# This file configures the workflow to run on the cray parts of WCOSS

platform: !Platform
  <<: *global_platform_common

  # Evaluate: this must be "false" to ensure disk space availability logic
  # is not run unless this file is for the current platform.
  Evaluate: false

  # name: the name of this platform; this must match what the underlying
  # scripts expect.
  name: sandbox

  # detect: this is a function that returns true iff the user is on GAEA
  # and false otherwise
  detect: True

  # skip_if_others_present: if this is true, and at least one other
  # platform is detected with this flag set to false, then skip this
  # platform
  skip_if_others_present: true

  # public_release_ics: location of input conditions that have been 
  # prepared for the public release.
  public_release_ics: /gpfs/hps3/emc/global/noscrub/emc.glopara/FV3GFS_V1_RELEASE/ICs

  # CHGRP_RSTPROD_COMMAND - this specifies the command to use to
  # restrict access to NOAA "rstprod" data restriction class.
  # This only used for observation processing, data assimilation, and
  # data assimilation archiving, which are not in the public release.
  CHGRP_RSTPROD_COMMAND: "chgrp rstprod"

  # NWPROD - location of the NCEP operational "nwprod" directory, which
  # only has meaning on the NCEP WCOSS machines.  It is used to get
  # the paths to certain programs and scripts.
  NWPROD: "/gpfs/hps/nco/ops/nwprod"

  # DMPDIR - location of the global dump data.  This is used by the observation
  # processing scripts, which are not included in the public release.
  DMPDIR: !calc doc.user_places.COMROOT
  #"/Users/jiankuang/Documents/Eclipse_workspace/ecfutils_007"

  # RTMFIX - location of the CRTM fixed data files used by the GSI data
  # assimilation.  The data assimilation is not included in this public release
  # so this path is unused.
  RTMFIX: "$CRTM_FIX"

  # BASE_SVN - a directory maintained by emc global model developers
  # that contains recent versions of source code and executables for
  # various subversion repositories.  This is used on some platforms
  # to find executables for this workflow.
  BASE_SVN: "/gpfs/hps3/emc/global/noscrub/emc.glopara/svn"

  # BASE_GIT - a directory maintained by emc global model developers
  # that contains recent versions of source code and executables for
  # various git repositories. This is used on some platforms to find
  # executables for this workflow.
  BASE_GIT: "/gpfs/hps3/emc/global/noscrub/emc.glopara/git"

  # config_base_extras - Additional configuration data to put in the
  # config.base file
  config_base_extras: "sandbox"
  #config_base_extras: |
  #  if [ -d /gpfs/tp1 ]; then
  #      export SITE="LUNA"
  #  elif [ -d /gpfs/gp1 ]; then
  #      export SITE="SURGE"
  #  fi

  metasched_more: {}

  partitions:
    Evaluate: false
    default_exclusive: !calc doc.platform.partitions.sandbox
    default_service: !calc doc.platform.partitions.sandbox
    default_shared: !calc doc.platform.partitions.sandbox
    default_bigmem: !calc doc.platform.partitions.sandbox
    sandbox:
      <<: *global_partition_common

      # specification - string to specify to the batch system to
      # request this partition.  Not relevant for WCOSS Cray
      specification: null

      # shared_accounting_ref - accounting settings for shared jobs
      shared_accounting_ref:
        project: !calc metasched.varref(doc.accounting.cpu_project)

      # service_accounting_ref - accounting settings for service jobs (jobs
      # that require tape or network access)
      service_accounting_ref:
        project: !calc metasched.varref(doc.accounting.cpu_project)

      # exclusive_accounting_ref - accounting settings for jobs that require
      # exclusive access to a node.
      exclusive_accounting_ref:
        project: !calc metasched.varref(doc.accounting.cpu_project)

      # Queues to use for each job type.  This logic automatically
      # switches between development queues on the backup machine and
      # development queues on the production machine based on whether the
      # /gpfs/hps2/ptmp is writable.
      shared_queue: dev
      service_queue: dev
      exclusive_queue: dev
      bigmem_queue: bigmem

      # Details about the scheduler on this cluster.
      scheduler_settings:
        scheduler_name: LSFAlps
        parallelism_name: LSFAlps
        node_type: generic
        physical_cores_per_node: 24
        logical_cpus_per_core: 2
        hyperthreading_allowed: true
        indent_text: "  "
        memory_per_node: !calc (64*1024)

      scheduler: !calc |
        tools.get_scheduler(scheduler_settings.scheduler_name, scheduler_settings)
      parallelism: !calc |
        tools.get_parallelism(scheduler_settings.parallelism_name, scheduler_settings)
      nodes: !calc |
        tools.node_tool_for(scheduler_settings.node_type, scheduler_settings)

  COMROOT: !expand "{doc.user_places.COMROOT}" 
  DATAROOT: !expand "{doc.user_places.DATAROOT}"
  EXPROOT: !expand "{doc.user_places.EXPROOT}"
